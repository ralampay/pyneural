{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "058e33bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features: 10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import math\n",
    "\n",
    "dataset = 'pageblocks'\n",
    "raw_file = \"./data/{}.csv\".format(dataset)\n",
    "\n",
    "df = pd.read_csv(raw_file, header=None)\n",
    "num_features = len(df.columns) - 1\n",
    "\n",
    "print(\"Number of Features: {}\".format(num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f3eddc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 5348\n",
      "Normal Count: 4863\n",
      "Anomaly Count: 485\n",
      "Contamination Ratio: 9.06881077038145\n"
     ]
    }
   ],
   "source": [
    "def partition_dataset(df, num_normal=20, num_anomalies=20):\n",
    "    df_n = df[df.iloc[:,-1] == 1].sample(num_normal)\n",
    "    df_a = df[df.iloc[:,-1] == -1].sample(num_anomalies)\n",
    "    \n",
    "    df.drop(df_n.index, inplace=True)\n",
    "    df.drop(df_a.index, inplace=True)\n",
    "    \n",
    "    frames = [df_n, df_a]\n",
    "    df_validation = pd.concat(frames)\n",
    "    \n",
    "    return df, df_validation\n",
    "\n",
    "df_training, df_validation = partition_dataset(df)\n",
    "\n",
    "total = len(df_training)\n",
    "num_normal = len(df_training[df_training.iloc[:,-1] == 1])\n",
    "num_anomalies = len(df_training[df_training.iloc[:,-1] == -1])\n",
    "contamination_ratio = (num_anomalies / total) * 100\n",
    "\n",
    "# Get the vector of values disregarding labels\n",
    "x_training = df_training.iloc[:,:-1].values\n",
    "x_validation = df_validation.iloc[:,:-1].values\n",
    "\n",
    "y_training = df_training.iloc[:,-1].values\n",
    "y_validation = df_validation.iloc[:,-1].values\n",
    "\n",
    "print(\"Total: {}\".format(total))\n",
    "print(\"Normal Count: {}\".format(num_normal))\n",
    "print(\"Anomaly Count: {}\".format(num_anomalies))\n",
    "print(\"Contamination Ratio: {}\".format(contamination_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbd2b0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.25      0.38        20\n",
      "           1       0.56      0.95      0.70        20\n",
      "\n",
      "    accuracy                           0.60        40\n",
      "   macro avg       0.70      0.60      0.54        40\n",
      "weighted avg       0.70      0.60      0.54        40\n",
      "\n",
      "MCC: 0.280056016805602\n",
      "F1: 0.38461538461538464\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.ecod import ECOD\n",
    "\n",
    "clf = ECOD()\n",
    "clf.fit(x_training)\n",
    "\n",
    "predictions = clf.predict(x_validation)\n",
    "predictions = np.where(predictions == 1, -1, predictions)\n",
    "predictions = np.where(predictions == 0, 1, predictions)\n",
    "\n",
    "cm = confusion_matrix(y_validation, predictions)\n",
    "tp = cm[0][0]\n",
    "tn = cm[1][1]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "\n",
    "mcc = ((tn * tp) - (fn * fp)) / math.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "f1 = tp / (tp + (0.5 * (fp + fn)))\n",
    "\n",
    "print(classification_report(y_validation, predictions))\n",
    "print(\"MCC: {}\".format(mcc))\n",
    "print(\"F1: {}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd13fb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.25      0.38        20\n",
      "           1       0.56      0.95      0.70        20\n",
      "\n",
      "    accuracy                           0.60        40\n",
      "   macro avg       0.70      0.60      0.54        40\n",
      "weighted avg       0.70      0.60      0.54        40\n",
      "\n",
      "MCC: 0.280056016805602\n",
      "F1: 0.38461538461538464\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.copod import COPOD\n",
    "\n",
    "clf = COPOD()\n",
    "clf.fit(x_training)\n",
    "\n",
    "predictions = clf.predict(x_validation)\n",
    "predictions = np.where(predictions == 1, -1, predictions)\n",
    "predictions = np.where(predictions == 0, 1, predictions)\n",
    "\n",
    "cm = confusion_matrix(y_validation, predictions)\n",
    "tp = cm[0][0]\n",
    "tn = cm[1][1]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "\n",
    "mcc = ((tn * tp) - (fn * fp)) / math.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "f1 = tp / (tp + (0.5 * (fp + fn)))\n",
    "\n",
    "print(classification_report(y_validation, predictions))\n",
    "print(\"MCC: {}\".format(mcc))\n",
    "print(\"F1: {}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa787e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Storing data to tensor...\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1070/1070 [00:01<00:00, 658.36it/s, loss=0.0101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 0.042496682813164786\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1070/1070 [00:01<00:00, 709.28it/s, loss=0.00644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 0.008213793458830495\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1070/1070 [00:01<00:00, 690.13it/s, loss=0.0043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 0.005428713019099521\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1070/1070 [00:01<00:00, 658.99it/s, loss=0.00288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ave Loss: 0.004207820525538681\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|███████████████████████████████████████████████████████████████████████████████████▌                                                                       | 577/1070 [00:00<00:00, 634.19it/s, loss=0.000763]"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '../pyneural/lib'))\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '../pyneural/modules'))\n",
    "\n",
    "from autoencoder import Autoencoder\n",
    "from train_autoencoder import TrainAutoencoder\n",
    "\n",
    "params = {\n",
    "    'layers':        [num_features, num_features - 3],\n",
    "    'batch_size':    5,\n",
    "    'model_file':    '/home/ralampay/workspace/pyneural/models/ae-{}.pth'.format(dataset),\n",
    "    'training_data': x_training,\n",
    "    'epochs':        100,\n",
    "    'learning_rate': 0.001\n",
    "}\n",
    "\n",
    "cmd = TrainAutoencoder(params)\n",
    "cmd.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64480ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_threshold_re import AutoThresholdRe\n",
    "import torch\n",
    "\n",
    "model = cmd.model\n",
    "x_tensor = torch.tensor(x_training).float()\n",
    "clf = AutoThresholdRe(x_tensor, model)\n",
    "clf.execute()\n",
    "\n",
    "print(\"Optimal Threshold: {}\".format(clf.optimal_threshold))\n",
    "\n",
    "predictions = clf.predict(torch.tensor(x_validation).float())\n",
    "\n",
    "cm = confusion_matrix(y_validation, predictions)\n",
    "tp = cm[0][0]\n",
    "tn = cm[1][1]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "\n",
    "mcc = ((tn * tp) - (fn * fp)) / math.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "f1 = tp / (tp + (0.5 * (fp + fn)))\n",
    "\n",
    "print(classification_report(y_validation, predictions))\n",
    "print(\"MCC: {}\".format(mcc))\n",
    "print(\"F1: {}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f2129c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffeae217",
   "metadata": {},
   "source": [
    "# Scoring an Anomaly Detection Dataset\n",
    "\n",
    "This is a proposal to provide a score for an anomaly detection dataset to measure its difficulty when attempting to perform anomaly detection methods against it. This is largely based on the paper https://arxiv.org/pdf/1503.01158v2.pdf. \n",
    "\n",
    "We focus on two major properties namely:\n",
    "1. Relative Frequency / Ratio of anomalies against data points in a dataset\n",
    "2. Semantic variation  / clusterdness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9f3f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and path relative to project\n",
    "from IPython.display import HTML, display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff749a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset './data/iris_setosa_anomaly.csv'...\n",
      "Reading chunk 1\n",
      "Done loading dataset...\n",
      "Input Dimensionality: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10908/2012265259.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append(chunk)\n"
     ]
    }
   ],
   "source": [
    "# Setup the dataset\n",
    "\n",
    "# Instantiate pandas DataFrame\n",
    "data = pd.DataFrame()\n",
    "\n",
    "# Chunk size for reading data\n",
    "chunksize = 10000\n",
    "\n",
    "# The reference to the dataset. Change this to \n",
    "dataset_file = './data/iris_setosa_anomaly.csv'\n",
    "\n",
    "print(\"Loading dataset '{}'...\".format(dataset_file))\n",
    "\n",
    "# Read each chunk and append to data frame\n",
    "for i, chunk in enumerate(pd.read_csv(dataset_file, header=None, chunksize=chunksize)):\n",
    "    print(\"Reading chunk %d\" % (i + 1))\n",
    "    data = data.append(chunk)\n",
    "\n",
    "print(\"Done loading dataset...\")\n",
    "    \n",
    "# Check for proper value of input dimensionality to be used by model\n",
    "input_dim = len(data.columns) - 1\n",
    "print(\"Input Dimensionality: %d\" % (input_dim))\n",
    "\n",
    "# Partition the data into positive_data and negative_data\n",
    "positive_data = data[data[input_dim] == 1].iloc[:,:input_dim]\n",
    "negative_data = data[data[input_dim] == -1].iloc[:,:input_dim].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6842112d",
   "metadata": {},
   "source": [
    "## Relative Frequency\n",
    "\n",
    "This simply the ratio of number of anomalies in the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13e5767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Frequency: 33.33333\n"
     ]
    }
   ],
   "source": [
    "# Divide the number of anomalies vs the entire length of the dataset\n",
    "# X: A pandas data frame\n",
    "def score_relative_frequency(X):\n",
    "    # Class column is always the last value\n",
    "    idx_class = len(X.columns) - 1\n",
    "    anomalies = X[X[idx_class] == -1]\n",
    "    \n",
    "    # Return the score in percentage format\n",
    "    return (len(anomalies) / len(X)) * 100\n",
    "\n",
    "print(\"Relative Frequency: %0.5f\" % (score_relative_frequency(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6f9491",
   "metadata": {},
   "source": [
    "## Semantic Variation\n",
    "\n",
    "A normalized clusterdness measure of given the following equation:\n",
    "\n",
    "$$\\log(\\frac{\\sigma^2_{n}}{\\sigma^2_{a}})$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\sigma^2_{n}$ is the variance of normal data\n",
    "* $\\sigma^2_{a}$ is the variance of anomaly data\n",
    "\n",
    "To deal with multi-dimensional data, we compute for the $\\sigma^2$ by taking the covariance matrix of the data $X$ using the equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{Var}(X) = \\mathbf{E}[(X - \\mathbf{E}(X))(X - \\mathbf{E}(X))^{T}]\n",
    "\\\\\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{Var}(X_1) & \\cdots &\n",
    "\\mathbf{Cov}(X_1, X_p) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\mathbf{Cov}{X_p, X_1} & \\cdots &\n",
    "\\mathbf{Var}(X_p)\n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "=\n",
    "\\frac{1}{n - 1}\n",
    "\\begin{bmatrix}\n",
    "\\sum_{i=1}^n(X_{i1} - \\hat{X}_{1})^2 & \\cdots &\n",
    "\\sum_{i=1}^n(X_{i1} - \\hat{X}_{1})(X_{ip} - \\hat{X}_{p}) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\sum_{i=1}^n(X_{ip} - \\hat{X}_{p})(X_{i1} - \\hat{X}_{1})  & \\cdots &\n",
    "\\sum_{i=1}^n(X_{ip} - \\hat{X}_{p})^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We then take trace of the covariance matrix to give us the overall variance:\n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\operatorname{tr}({\\mathbf{Var}(X)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3aeb1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic variation: 0.71245\n"
     ]
    }
   ],
   "source": [
    "def score_semantic_variation(X):\n",
    "    idx_class = len(X.columns) - 1\n",
    "    \n",
    "    # Partition the data into positive_data and negative_data\n",
    "    positive_data = X[X[idx_class] == 1].iloc[:,:idx_class]\n",
    "    negative_data = X[X[idx_class] == -1].iloc[:,:idx_class]\n",
    "    \n",
    "    var_n = np.trace(positive_data.cov().values)\n",
    "    var_a = np.trace(negative_data.cov().values)\n",
    "    \n",
    "    return np.log(var_n / var_a)\n",
    "    \n",
    "print(\"Semantic variation: %0.5f\" % (score_semantic_variation(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b2ff82",
   "metadata": {},
   "source": [
    "## Test against public datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61a2b4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset '/home/ralampay/workspace/pyneural/notebooks/partitions/annthyroid-training-3251546f-bb22-48d8-ad40-13d51e75bf5f.csv'...\n",
      "Loading dataset '/home/ralampay/workspace/pyneural/notebooks/partitions/pageblocks-training-19a7bd3b-9640-4a5b-9ca2-32eb4dc08194.csv'...\n",
      "Loading dataset '/home/ralampay/workspace/pyneural/notebooks/partitions/creditcardfraud-training-1b5b3362-8ae5-419f-8a93-bec611aae5ac.csv'...\n",
      "Loading dataset '/home/ralampay/workspace/pyneural/notebooks/partitions/kddcup99-training-1ee21411-1cae-43a4-9575-f94c4bdf7ae4.csv'...\n",
      "Loading dataset '/home/ralampay/workspace/pyneural/notebooks/partitions/shuttle-training-eed45643-1cf0-4cd3-9ac9-b7da0538565e.csv'...\n",
      "Loading dataset '/home/ralampay/workspace/pyneural/notebooks/partitions/backdoor-training-155cbadd-b98b-480e-b3ae-fc172b620dba.csv'...\n",
      "Loading dataset '/home/ralampay/workspace/pyneural/notebooks/partitions/donors-training-b2881566-edd9-4d15-a710-b0927d951d52.csv'...\n",
      "Loading dataset '/home/ralampay/workspace/pyneural/notebooks/partitions/magic04-training-ba90e6a7-3fd6-405f-9aed-54e39864582f.csv'...\n",
      "Loading dataset '/home/ralampay/workspace/pyneural/notebooks/partitions/mammography-training-ea58423d-2f5d-4c25-b5dd-4e524b1ad202.csv'...\n",
      "Loading dataset '/home/ralampay/workspace/pyneural/notebooks/partitions/waveform-training-7aa452b3-6387-4fd2-842a-696065e975d7.csv'...\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "    { \"name\": \"annthyroid\", \"location\": \"/home/ralampay/workspace/pyneural/notebooks/partitions/annthyroid-training-3251546f-bb22-48d8-ad40-13d51e75bf5f.csv\" },\n",
    "    { \"name\": \"pageblocks\", \"location\": \"/home/ralampay/workspace/pyneural/notebooks/partitions/pageblocks-training-19a7bd3b-9640-4a5b-9ca2-32eb4dc08194.csv\" },\n",
    "    { \"name\": \"creditcardfraud\", \"location\": \"/home/ralampay/workspace/pyneural/notebooks/partitions/creditcardfraud-training-1b5b3362-8ae5-419f-8a93-bec611aae5ac.csv\" },\n",
    "    { \"name\": \"kddcup99\", \"location\": \"/home/ralampay/workspace/pyneural/notebooks/partitions/kddcup99-training-1ee21411-1cae-43a4-9575-f94c4bdf7ae4.csv\" },\n",
    "    { \"name\": \"shuttle\", \"location\": \"/home/ralampay/workspace/pyneural/notebooks/partitions/shuttle-training-eed45643-1cf0-4cd3-9ac9-b7da0538565e.csv\" },\n",
    "    { \"name\": \"backdoor\", \"location\": \"/home/ralampay/workspace/pyneural/notebooks/partitions/backdoor-training-155cbadd-b98b-480e-b3ae-fc172b620dba.csv\" },\n",
    "    { \"name\": \"donors\", \"location\": \"/home/ralampay/workspace/pyneural/notebooks/partitions/donors-training-b2881566-edd9-4d15-a710-b0927d951d52.csv\" },\n",
    "    { \"name\": \"magic04\", \"location\": \"/home/ralampay/workspace/pyneural/notebooks/partitions/magic04-training-ba90e6a7-3fd6-405f-9aed-54e39864582f.csv\" },\n",
    "    { \"name\": \"mammography\", \"location\": \"/home/ralampay/workspace/pyneural/notebooks/partitions/mammography-training-ea58423d-2f5d-4c25-b5dd-4e524b1ad202.csv\" },\n",
    "    { \"name\": \"waveform\", \"location\": \"/home/ralampay/workspace/pyneural/notebooks/partitions/waveform-training-7aa452b3-6387-4fd2-842a-696065e975d7.csv\" }\n",
    "]\n",
    "\n",
    "scores = [[\"Dataset\", \"Relative Frequency\", \"Semantic Variation\"]]\n",
    "\n",
    "for o in datasets:\n",
    "    # Instantiate pandas DataFrame\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    # Chunk size for reading data\n",
    "    chunksize = 10000\n",
    "\n",
    "    # The reference to the dataset. Change this to \n",
    "    dataset_file = o[\"location\"]\n",
    "\n",
    "    print(\"Loading dataset '{}'...\".format(dataset_file))\n",
    "\n",
    "    # Read each chunk and append to data frame\n",
    "    for i, chunk in enumerate(pd.read_csv(dataset_file, header=None, chunksize=chunksize)):\n",
    "        #print(\"Reading chunk %d\" % (i + 1))\n",
    "        data = pd.concat([data, chunk])\n",
    "\n",
    "    #print(\"Done loading dataset %s...\" % (o[\"name\"]))\n",
    "    input_dim = len(data.columns) - 1\n",
    "    \n",
    "    score_rf = score_relative_frequency(data)\n",
    "    score_sv = score_semantic_variation(data)\n",
    "    scores.append([\n",
    "        o[\"name\"],\n",
    "        score_rf,\n",
    "        score_sv\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1a39340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Dataset        </td><td>Relative Frequency </td><td>Semantic Variation   </td></tr>\n",
       "<tr><td>annthyroid     </td><td>6.2                </td><td>0.08383434074608195  </td></tr>\n",
       "<tr><td>pageblocks     </td><td>7.806476484194294  </td><td>4.53706272063149     </td></tr>\n",
       "<tr><td>creditcardfraud</td><td>0.13773378729265268</td><td>-0.05594329526420958 </td></tr>\n",
       "<tr><td>kddcup99       </td><td>0.20037988686885552</td><td>11.002328556599126   </td></tr>\n",
       "<tr><td>shuttle        </td><td>6.918779544477868  </td><td>5.143372055024244    </td></tr>\n",
       "<tr><td>backdoor       </td><td>2.3431340600658053 </td><td>-0.26974687256027857 </td></tr>\n",
       "<tr><td>donors         </td><td>5.913174378074899  </td><td>-0.001947262023485022</td></tr>\n",
       "<tr><td>magic04        </td><td>35.00531349628056  </td><td>1.2236408577475006   </td></tr>\n",
       "<tr><td>mammography    </td><td>1.456796867886734  </td><td>-0.03327060424053471 </td></tr>\n",
       "<tr><td>waveform       </td><td>2.3221634332745444 </td><td>7.010442339290185    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "'<table>\\n<tbody>\\n<tr><td>Dataset        </td><td>Relative Frequency </td><td>Semantic Variation   </td></tr>\\n<tr><td>annthyroid     </td><td>6.2                </td><td>0.08383434074608195  </td></tr>\\n<tr><td>pageblocks     </td><td>7.806476484194294  </td><td>4.53706272063149     </td></tr>\\n<tr><td>creditcardfraud</td><td>0.13773378729265268</td><td>-0.05594329526420958 </td></tr>\\n<tr><td>kddcup99       </td><td>0.20037988686885552</td><td>11.002328556599126   </td></tr>\\n<tr><td>shuttle        </td><td>6.918779544477868  </td><td>5.143372055024244    </td></tr>\\n<tr><td>backdoor       </td><td>2.3431340600658053 </td><td>-0.26974687256027857 </td></tr>\\n<tr><td>donors         </td><td>5.913174378074899  </td><td>-0.001947262023485022</td></tr>\\n<tr><td>magic04        </td><td>35.00531349628056  </td><td>1.2236408577475006   </td></tr>\\n<tr><td>mammography    </td><td>1.456796867886734  </td><td>-0.03327060424053471 </td></tr>\\n<tr><td>waveform       </td><td>2.3221634332745444 </td><td>7.010442339290185    </td></tr>\\n</tbody>\\n</table>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display result in tabular format\n",
    "tabulate.tabulate(scores, tablefmt='html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
